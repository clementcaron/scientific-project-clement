{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc1ce4e",
   "metadata": {},
   "source": [
    "# LLM Reasoning Framework Comparison Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of three reasoning frameworks for LLM-based agents:\n",
    "- **ReAct** (Reasoning + Acting): Alternates between reasoning and action steps\n",
    "- **Chain-of-Thought (CoT)**: Uses linear step-by-step reasoning\n",
    "- **Tree-of-Thoughts (ToT)**: Explores multiple reasoning paths and selects the best\n",
    "\n",
    "## Evaluation Tasks\n",
    "The frameworks are evaluated on three distinct task types:\n",
    "1. **Code Generation**: Implement Conway's Game of Life in Python\n",
    "2. **Itinerary Planning**: Generate optimized travel routes with constraints  \n",
    "3. **Procedure Structuring**: Transform vague instructions into clear procedures\n",
    "\n",
    "## Experimental Setup\n",
    "- Each framework tested on 3 examples per task type (9 total tasks)\n",
    "- Each task executed 3 times to measure consistency\n",
    "- Standardized prompts and temperature (0.3) across all frameworks\n",
    "- Metrics: token usage, execution time, validation scores, success rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5356e",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langchain langchain-google-genai langchain-openai langchain-mistralai\n",
    "# !pip install python-dotenv pandas numpy matplotlib seaborn plotly streamlit gradio\n",
    "# !pip install psutil tiktoken jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Data analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# LLM and framework imports\n",
    "from langchain.llms.base import LLM\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# System monitoring\n",
    "import psutil\n",
    "\n",
    "# Add project modules to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dad90c",
   "metadata": {},
   "source": [
    "## 2. Define Prompt Templates for Each Task Category\n",
    "\n",
    "Standardized prompt templates ensure consistency across all reasoning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplates:\n",
    "    \"\"\"Standardized prompt templates for different reasoning frameworks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_react_template(task_prompt: str, task_type: str) -> str:\n",
    "        \"\"\"ReAct framework template with Thought-Action-Observation structure.\"\"\"\n",
    "        return f\"\"\"You are solving a {task_type} task. Use the ReAct framework: alternate between Thought and Action steps.\n",
    "\n",
    "Task: {task_prompt}\n",
    "\n",
    "Follow this exact format:\n",
    "Thought: [Your reasoning about what to do next]\n",
    "Action: [The specific action you're taking]\n",
    "Observation: [What you learned from the action]\n",
    "\n",
    "Continue this Thought-Action-Observation cycle until you reach a final answer.\n",
    "When you have the complete solution, end with:\n",
    "Final Answer: [Your complete solution]\n",
    "\n",
    "Important guidelines:\n",
    "- For code generation: Think through the algorithm step by step, then implement incrementally\n",
    "- For itinerary planning: Consider constraints, calculate distances/times, optimize step by step  \n",
    "- For procedure structuring: Analyze the vague instructions, identify key steps, organize logically\n",
    "\n",
    "Begin:\n",
    "\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cot_template(task_prompt: str, task_type: str) -> str:\n",
    "        \"\"\"Chain-of-Thought template with linear step-by-step reasoning.\"\"\"\n",
    "        return f\"\"\"You are solving a {task_type} task. Use Chain-of-Thought reasoning: break down the problem into clear, logical steps.\n",
    "\n",
    "Task: {task_prompt}\n",
    "\n",
    "Think through this step by step:\n",
    "\n",
    "Step 1: [Understand the problem and identify key requirements]\n",
    "Step 2: [Break down the problem into smaller components]\n",
    "Step 3: [Plan your approach or algorithm]\n",
    "Step 4: [Implement/work through the first part]\n",
    "Step 5: [Continue with subsequent parts]\n",
    "...\n",
    "Step N: [Complete the solution and verify]\n",
    "\n",
    "Final Solution: [Your complete answer]\n",
    "\n",
    "Guidelines for each task type:\n",
    "- Code Generation: Analyze requirements → Design algorithm → Implement incrementally → Test logic\n",
    "- Itinerary Planning: Parse constraints → Research options → Calculate costs/times → Optimize route\n",
    "- Procedure Structuring: Identify core objectives → Break into logical steps → Sequence properly → Add details\n",
    "\n",
    "Let's work through this systematically:\n",
    "\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tot_template(task_prompt: str, task_type: str, num_branches: int = 3) -> str:\n",
    "        \"\"\"Tree-of-Thoughts template with multiple path exploration.\"\"\"\n",
    "        return f\"\"\"You are solving a {task_type} task using Tree-of-Thoughts reasoning. Explore multiple approaches and select the best one.\n",
    "\n",
    "Task: {task_prompt}\n",
    "\n",
    "Follow this structure:\n",
    "\n",
    "APPROACH GENERATION:\n",
    "Generate {num_branches} different approaches to solve this problem:\n",
    "\n",
    "Approach 1: [Describe first potential method]\n",
    "Approach 2: [Describe second potential method]  \n",
    "Approach 3: [Describe third potential method]\n",
    "\n",
    "APPROACH EVALUATION:\n",
    "Evaluate each approach:\n",
    "\n",
    "Approach 1 Assessment: [Pros, cons, feasibility - Rate 1-10]\n",
    "Approach 2 Assessment: [Pros, cons, feasibility - Rate 1-10]\n",
    "Approach 3 Assessment: [Pros, cons, feasibility - Rate 1-10]\n",
    "\n",
    "BEST APPROACH SELECTION:\n",
    "Selected Approach: [Choose the highest-rated approach and explain why]\n",
    "\n",
    "DETAILED EXECUTION:\n",
    "Now implement the selected approach step by step:\n",
    "Step 1: [First implementation step]\n",
    "Step 2: [Second implementation step]\n",
    "...\n",
    "Step N: [Final step]\n",
    "\n",
    "Final Solution: [Complete solution using the best approach]\n",
    "\n",
    "Task-specific considerations:\n",
    "- Code Generation: Consider different algorithms, data structures, complexity trade-offs\n",
    "- Itinerary Planning: Explore different route options, transportation modes, optimization criteria\n",
    "- Procedure Structuring: Try different organizational frameworks, sequencing approaches\n",
    "\n",
    "Begin exploration:\n",
    "\"\"\"\n",
    "\n",
    "# Test the templates\n",
    "print(\"Prompt templates defined successfully!\")\n",
    "print(f\"ReAct template length: {len(PromptTemplates.get_react_template('test task', 'test_type'))}\")\n",
    "print(f\"CoT template length: {len(PromptTemplates.get_cot_template('test task', 'test_type'))}\")\n",
    "print(f\"ToT template length: {len(PromptTemplates.get_tot_template('test task', 'test_type'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f7687",
   "metadata": {},
   "source": [
    "## 3. Implement Agent Classes: ReAct, CoT, ToT\n",
    "\n",
    "Agent classes implement the specific reasoning frameworks using LangChain for structured behavior and prompt logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840df384",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExecutionMetrics:\n",
    "    \"\"\"Metrics collected during agent execution.\"\"\"\n",
    "    tokens_used: int\n",
    "    execution_time: float\n",
    "    memory_usage: float\n",
    "    reasoning_steps: int\n",
    "    final_answer: str\n",
    "    intermediate_steps: List[str]\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all reasoning framework agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: LLM, temperature: float = 0.3, max_tokens: int = 2048):\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.framework_name = self.__class__.__name__\n",
    "        \n",
    "    def execute_task(self, task_prompt: str, task_type: str) -> ExecutionMetrics:\n",
    "        \"\"\"Execute a task using the specific reasoning framework.\"\"\"\n",
    "        full_prompt = self.get_framework_prompt(task_prompt, task_type)\n",
    "        \n",
    "        def _run_task():\n",
    "            response = self.llm.invoke(full_prompt)\n",
    "            return response\n",
    "        \n",
    "        result, exec_time, memory_usage, success, error = self._measure_execution(_run_task)\n",
    "        \n",
    "        if not success:\n",
    "            return ExecutionMetrics(\n",
    "                tokens_used=0,\n",
    "                execution_time=exec_time,\n",
    "                memory_usage=memory_usage,\n",
    "                reasoning_steps=0,\n",
    "                final_answer=\"\",\n",
    "                intermediate_steps=[],\n",
    "                success=False,\n",
    "                error_message=error\n",
    "            )\n",
    "        \n",
    "        # Parse response\n",
    "        reasoning_steps = self._extract_reasoning_steps(result)\n",
    "        final_answer = self._extract_final_answer(result)\n",
    "        tokens_used = self._count_tokens(full_prompt + str(result))\n",
    "        \n",
    "        return ExecutionMetrics(\n",
    "            tokens_used=tokens_used,\n",
    "            execution_time=exec_time,\n",
    "            memory_usage=memory_usage,\n",
    "            reasoning_steps=len(reasoning_steps),\n",
    "            final_answer=final_answer,\n",
    "            intermediate_steps=reasoning_steps,\n",
    "            success=True\n",
    "        )\n",
    "    \n",
    "    def get_framework_prompt(self, task_prompt: str, task_type: str) -> str:\n",
    "        \"\"\"Generate the framework-specific prompt.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _measure_execution(self, func, *args, **kwargs) -> tuple:\n",
    "        \"\"\"Measure execution time and memory usage.\"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            success = True\n",
    "            error = None\n",
    "        except Exception as e:\n",
    "            result = None\n",
    "            success = False\n",
    "            error = str(e)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        memory_usage = end_memory - start_memory\n",
    "        \n",
    "        return result, execution_time, memory_usage, success, error\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count (simplified).\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def _extract_reasoning_steps(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract reasoning steps from the response.\"\"\"\n",
    "        return [response]\n",
    "    \n",
    "    def _extract_final_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract the final answer from the response.\"\"\"\n",
    "        return response.strip()\n",
    "\n",
    "print(\"Base agent class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActAgent(BaseAgent):\n",
    "    \"\"\"ReAct agent that alternates between reasoning and acting.\"\"\"\n",
    "    \n",
    "    def get_framework_prompt(self, task_prompt: str, task_type: str) -> str:\n",
    "        \"\"\"Generate ReAct-specific prompt.\"\"\"\n",
    "        return PromptTemplates.get_react_template(task_prompt, task_type)\n",
    "    \n",
    "    def _extract_reasoning_steps(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract Thought-Action-Observation cycles from ReAct response.\"\"\"\n",
    "        steps = []\n",
    "        \n",
    "        # Find all Thought-Action-Observation patterns\n",
    "        thought_pattern = r\"Thought:\\s*(.*?)(?=\\nAction:|$)\"\n",
    "        action_pattern = r\"Action:\\s*(.*?)(?=\\nObservation:|$)\"\n",
    "        observation_pattern = r\"Observation:\\s*(.*?)(?=\\nThought:|Final Answer:|$)\"\n",
    "        \n",
    "        thoughts = re.findall(thought_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        actions = re.findall(action_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        observations = re.findall(observation_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        # Combine into reasoning steps\n",
    "        max_len = max(len(thoughts), len(actions), len(observations))\n",
    "        for i in range(max_len):\n",
    "            step_parts = []\n",
    "            if i < len(thoughts):\n",
    "                step_parts.append(f\"Thought: {thoughts[i].strip()}\")\n",
    "            if i < len(actions):\n",
    "                step_parts.append(f\"Action: {actions[i].strip()}\")\n",
    "            if i < len(observations):\n",
    "                step_parts.append(f\"Observation: {observations[i].strip()}\")\n",
    "            \n",
    "            if step_parts:\n",
    "                steps.append(\" | \".join(step_parts))\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def _extract_final_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract the final answer from ReAct response.\"\"\"\n",
    "        final_answer_pattern = r\"Final Answer:\\s*(.*?)(?:\\n|$)\"\n",
    "        match = re.search(final_answer_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        return lines[-1] if lines else \"\"\n",
    "\n",
    "\n",
    "class CoTAgent(BaseAgent):\n",
    "    \"\"\"Chain-of-Thought agent that uses linear step-by-step reasoning.\"\"\"\n",
    "    \n",
    "    def get_framework_prompt(self, task_prompt: str, task_type: str) -> str:\n",
    "        \"\"\"Generate CoT-specific prompt.\"\"\"\n",
    "        return PromptTemplates.get_cot_template(task_prompt, task_type)\n",
    "    \n",
    "    def _extract_reasoning_steps(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract numbered steps from CoT response.\"\"\"\n",
    "        steps = []\n",
    "        \n",
    "        # Find all numbered steps\n",
    "        step_pattern = r\"Step\\s*(\\d+):\\s*(.*?)(?=\\nStep\\s*\\d+:|Final Solution:|$)\"\n",
    "        matches = re.findall(step_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        for step_num, step_content in matches:\n",
    "            steps.append(f\"Step {step_num}: {step_content.strip()}\")\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def _extract_final_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract the final solution from CoT response.\"\"\"\n",
    "        final_patterns = [\n",
    "            r\"Final Solution:\\s*(.*?)(?:\\n|$)\",\n",
    "            r\"Final Answer:\\s*(.*?)(?:\\n|$)\",\n",
    "            r\"Solution:\\s*(.*?)(?:\\n|$)\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in final_patterns:\n",
    "            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        return lines[-1] if lines else \"\"\n",
    "\n",
    "\n",
    "class ToTAgent(BaseAgent):\n",
    "    \"\"\"Tree-of-Thoughts agent that explores multiple reasoning paths.\"\"\"\n",
    "    \n",
    "    def get_framework_prompt(self, task_prompt: str, task_type: str) -> str:\n",
    "        \"\"\"Generate ToT-specific prompt.\"\"\"\n",
    "        return PromptTemplates.get_tot_template(task_prompt, task_type)\n",
    "    \n",
    "    def _extract_reasoning_steps(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract the different phases of ToT reasoning.\"\"\"\n",
    "        steps = []\n",
    "        \n",
    "        # Extract approaches\n",
    "        approach_pattern = r\"Approach\\s*(\\d+):\\s*(.*?)(?=\\nApproach\\s*\\d+:|APPROACH EVALUATION:|$)\"\n",
    "        approaches = re.findall(approach_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        for approach_num, approach_content in approaches:\n",
    "            steps.append(f\"Generated Approach {approach_num}: {approach_content.strip()}\")\n",
    "        \n",
    "        # Extract evaluations\n",
    "        eval_pattern = r\"Approach\\s*(\\d+)\\s*Assessment:\\s*(.*?)(?=\\nApproach\\s*\\d+\\s*Assessment:|BEST APPROACH SELECTION:|$)\"\n",
    "        evaluations = re.findall(eval_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        for eval_num, eval_content in evaluations:\n",
    "            steps.append(f\"Evaluated Approach {eval_num}: {eval_content.strip()}\")\n",
    "        \n",
    "        # Extract selected approach\n",
    "        selection_pattern = r\"Selected Approach:\\s*(.*?)(?=\\nDETAILED EXECUTION:|$)\"\n",
    "        selection_match = re.search(selection_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        if selection_match:\n",
    "            steps.append(f\"Selected Best Approach: {selection_match.group(1).strip()}\")\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def _extract_final_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract the final solution from ToT response.\"\"\"\n",
    "        final_pattern = r\"Final Solution:\\s*(.*?)(?:\\n|$)\"\n",
    "        match = re.search(final_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        return lines[-1] if lines else \"\"\n",
    "\n",
    "print(\"All agent classes implemented successfully!\")\n",
    "print(\"Available agents: ReActAgent, CoTAgent, ToTAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a72fa",
   "metadata": {},
   "source": [
    "## 4. Define Task Examples for Each Task Type\n",
    "\n",
    "Three example tasks for each category (code generation, itinerary planning, procedure structuring) organized for repeated evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b157f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"A single task instance.\"\"\"\n",
    "    id: str\n",
    "    task_type: str\n",
    "    title: str\n",
    "    prompt: str\n",
    "    expected_output_type: str\n",
    "    validation_criteria: List[str]\n",
    "\n",
    "\n",
    "class TaskGenerator:\n",
    "    \"\"\"Generates tasks for different categories.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_code_generation_tasks() -> List[Task]:\n",
    "        \"\"\"Generate code generation tasks.\"\"\"\n",
    "        return [\n",
    "            Task(\n",
    "                id=\"code_001\",\n",
    "                task_type=\"code_generation\",\n",
    "                title=\"Conway's Game of Life\",\n",
    "                prompt=\"\"\"Implement Conway's Game of Life in Python. Requirements:\n",
    "- Create a Grid class that can initialize with a given size\n",
    "- Implement the four rules of Conway's Game of Life:\n",
    "  1. Any live cell with 2-3 live neighbors survives\n",
    "  2. Any dead cell with exactly 3 live neighbors becomes alive\n",
    "  3. All other live cells die, all other dead cells stay dead\n",
    "- Include methods to: display the grid, advance one generation, count live neighbors\n",
    "- Provide a simple test case with a known pattern (e.g., blinker or glider)\n",
    "- Make it runnable as a script that shows several generations\"\"\",\n",
    "                expected_output_type=\"python_code\",\n",
    "                validation_criteria=[\n",
    "                    \"Contains a Grid class\",\n",
    "                    \"Implements the four rules correctly\",\n",
    "                    \"Has neighbor counting logic\",\n",
    "                    \"Includes display functionality\",\n",
    "                    \"Provides a test case\"\n",
    "                ]\n",
    "            ),\n",
    "            \n",
    "            Task(\n",
    "                id=\"code_002\", \n",
    "                task_type=\"code_generation\",\n",
    "                title=\"Binary Search Tree Implementation\",\n",
    "                prompt=\"\"\"Create a Binary Search Tree (BST) implementation in Python. Requirements:\n",
    "- Implement a Node class and BST class\n",
    "- Include methods: insert, search, delete, inorder_traversal\n",
    "- Handle edge cases (empty tree, single node, etc.)\n",
    "- Implement tree balancing check method\n",
    "- Add a method to find the minimum and maximum values\n",
    "- Include comprehensive test cases showing insertion, deletion, and traversal\n",
    "- Make the code well-documented with docstrings\"\"\",\n",
    "                expected_output_type=\"python_code\", \n",
    "                validation_criteria=[\n",
    "                    \"Contains Node and BST classes\",\n",
    "                    \"Implements all required methods\",\n",
    "                    \"Handles edge cases\",\n",
    "                    \"Includes balancing check\",\n",
    "                    \"Has min/max finding methods\",\n",
    "                    \"Contains test cases\"\n",
    "                ]\n",
    "            ),\n",
    "            \n",
    "            Task(\n",
    "                id=\"code_003\",\n",
    "                task_type=\"code_generation\", \n",
    "                title=\"Text Analysis Tool\",\n",
    "                prompt=\"\"\"Build a text analysis tool in Python that processes a text file. Requirements:\n",
    "- Read text from a file or string input\n",
    "- Count: words, sentences, paragraphs, characters\n",
    "- Find: most common words, average word length, reading time estimate\n",
    "- Implement sentiment analysis (simple positive/negative word counting)\n",
    "- Create word frequency distribution\n",
    "- Generate a summary report in both text and JSON formats\n",
    "- Handle different file encodings and basic error cases\n",
    "- Include a command-line interface\"\"\",\n",
    "                expected_output_type=\"python_code\",\n",
    "                validation_criteria=[\n",
    "                    \"Reads text input properly\",\n",
    "                    \"Implements all counting features\",\n",
    "                    \"Has word frequency analysis\", \n",
    "                    \"Includes sentiment analysis\",\n",
    "                    \"Outputs in multiple formats\",\n",
    "                    \"Has CLI interface\"\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_itinerary_planning_tasks() -> List[Task]:\n",
    "        \"\"\"Generate itinerary planning tasks.\"\"\"\n",
    "        return [\n",
    "            Task(\n",
    "                id=\"itin_001\",\n",
    "                task_type=\"itinerary_planning\",\n",
    "                title=\"European City Tour\",\n",
    "                prompt=\"\"\"Plan a 7-day European tour itinerary. Constraints:\n",
    "- Budget: $2000 USD total\n",
    "- Start and end in London\n",
    "- Must visit: Paris, Amsterdam, Berlin\n",
    "- Interests: Museums, historical sites, local cuisine\n",
    "- Transportation: Train preferred, flights if necessary\n",
    "- Accommodation: Mid-range hotels/hostels\n",
    "- Travel dates: flexible, summer preferred\n",
    "- Create day-by-day schedule with specific activities, costs, and travel times\n",
    "- Include backup options for bad weather\"\"\",\n",
    "                expected_output_type=\"structured_itinerary\",\n",
    "                validation_criteria=[\n",
    "                    \"Covers all 7 days\",\n",
    "                    \"Visits all required cities\",\n",
    "                    \"Stays within budget\",\n",
    "                    \"Includes specific activities\",\n",
    "                    \"Shows transportation details\",\n",
    "                    \"Has cost breakdown\"\n",
    "                ]\n",
    "            ),\n",
    "            \n",
    "            Task(\n",
    "                id=\"itin_002\",\n",
    "                task_type=\"itinerary_planning\", \n",
    "                title=\"Business Trip Optimization\",\n",
    "                prompt=\"\"\"Optimize a business trip itinerary. Constraints:\n",
    "- Duration: 3 days\n",
    "- Cities: New York, Philadelphia, Washington DC\n",
    "- Meetings scheduled: NYC (Day 1, 2pm), Philadelphia (Day 2, 10am), DC (Day 3, 11am)\n",
    "- Requirements: Minimize travel time, stay near meeting locations\n",
    "- Budget: $1500 for accommodation and transport\n",
    "- Need reliable internet for virtual meetings\n",
    "- Prefer train travel when possible\n",
    "- Include time for one business dinner and one cultural activity\"\"\",\n",
    "                expected_output_type=\"structured_itinerary\",\n",
    "                validation_criteria=[\n",
    "                    \"Accommodates all meetings\",\n",
    "                    \"Minimizes travel time\",\n",
    "                    \"Stays within budget\", \n",
    "                    \"Includes business and cultural activities\",\n",
    "                    \"Shows transportation logistics\"\n",
    "                ]\n",
    "            ),\n",
    "            \n",
    "            Task(\n",
    "                id=\"itin_003\",\n",
    "                task_type=\"itinerary_planning\",\n",
    "                title=\"Family Vacation Planning\", \n",
    "                prompt=\"\"\"Plan a family vacation for 2 adults and 2 children (ages 8, 12). Constraints:\n",
    "- Destination: Orlando, Florida\n",
    "- Duration: 5 days\n",
    "- Budget: $3000 total\n",
    "- Must include: Disney World (2 days), Universal Studios (1 day)\n",
    "- Requirements: Family-friendly restaurants, nearby accommodation\n",
    "- Special needs: One child has food allergies (nuts)\n",
    "- Transportation: Flying from Chicago\n",
    "- Want to include one non-theme park activity\n",
    "- Create detailed daily plans with timing and alternatives\"\"\",\n",
    "                expected_output_type=\"structured_itinerary\",\n",
    "                validation_criteria=[\n",
    "                    \"Accommodates family needs\",\n",
    "                    \"Includes all required attractions\",\n",
    "                    \"Considers food allergies\",\n",
    "                    \"Stays within budget\",\n",
    "                    \"Has detailed daily schedules\"\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    @staticmethod \n",
    "    def get_procedure_structuring_tasks() -> List[Task]:\n",
    "        \"\"\"Generate procedure structuring tasks.\"\"\"\n",
    "        return [\n",
    "            Task(\n",
    "                id=\"proc_001\",\n",
    "                task_type=\"procedure_structuring\",\n",
    "                title=\"Software Deployment Process\",\n",
    "                prompt=\"\"\"Restructure this vague deployment instruction into clear steps:\n",
    "\"Deploy the new version to production. Make sure to backup everything first and test it. Don't forget about the database migration and updating the configs. If something breaks, roll back. Also notify the team when done and update documentation.\"\n",
    "\n",
    "Transform this into a detailed, step-by-step procedure that could be followed by any team member.\"\"\",\n",
    "                expected_output_type=\"structured_procedure\",\n",
    "                validation_criteria=[\n",
    "                    \"Clear sequential steps\",\n",
    "                    \"Includes all mentioned tasks\",\n",
    "                    \"Has verification points\",\n",
    "                    \"Covers error handling\",\n",
    "                    \"Specifies responsibilities\"\n",
    "                ]\n",
    "            ),\n",
    "            \n",
    "            Task(\n",
    "                id=\"proc_002\",\n",
    "                task_type=\"procedure_structuring\",\n",
    "                title=\"Customer Onboarding Process\",\n",
    "                prompt=\"\"\"Convert this unclear onboarding description into a structured procedure:\n",
    "\"New customers need to sign up, verify their info, get set up with accounts, learn how to use the system, and start their subscription. Someone should welcome them and make sure they understand everything. We also need to collect their preferences and set up their profile properly.\"\n",
    "\n",
    "Create a comprehensive onboarding procedure with clear steps, timelines, and responsibilities.\"\"\",\n",
    "                expected_output_type=\"structured_procedure\", \n",
    "                validation_criteria=[\n",
    "                    \"Logical step sequence\",\n",
    "                    \"Clear timelines\",\n",
    "                    \"Defined responsibilities\",\n",
    "                    \"Covers all mentioned elements\",\n",
    "                    \"Includes quality checkpoints\"\n",
    "                ]\n",
    "            ),\n",
    "            \n",
    "            Task(\n",
    "                id=\"proc_003\", \n",
    "                task_type=\"procedure_structuring\",\n",
    "                title=\"Emergency Response Protocol\",\n",
    "                prompt=\"\"\"Reorganize this emergency response description into a clear protocol:\n",
    "\"When there's a system outage, everyone needs to know what to do. First figure out what's wrong, then fix it, and tell people about it. Make sure to keep track of what happened and write it down later. Someone should be in charge and coordinate everything. Don't panic and follow the escalation rules.\"\n",
    "\n",
    "Transform this into a detailed emergency response protocol with specific roles, actions, and communication procedures.\"\"\",\n",
    "                expected_output_type=\"structured_procedure\",\n",
    "                validation_criteria=[\n",
    "                    \"Clear command structure\",\n",
    "                    \"Specific action steps\", \n",
    "                    \"Communication protocols\",\n",
    "                    \"Documentation requirements\",\n",
    "                    \"Escalation procedures\"\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all_tasks(cls) -> Dict[str, List[Task]]:\n",
    "        \"\"\"Get all tasks organized by type.\"\"\"\n",
    "        return {\n",
    "            \"code_generation\": cls.get_code_generation_tasks(),\n",
    "            \"itinerary_planning\": cls.get_itinerary_planning_tasks(), \n",
    "            \"procedure_structuring\": cls.get_procedure_structuring_tasks()\n",
    "        }\n",
    "\n",
    "# Initialize task generator and get all tasks\n",
    "task_generator = TaskGenerator()\n",
    "all_tasks = task_generator.get_all_tasks()\n",
    "\n",
    "print(\"Task definitions loaded successfully!\")\n",
    "for task_type, tasks in all_tasks.items():\n",
    "    print(f\"  {task_type}: {len(tasks)} tasks\")\n",
    "    for task in tasks:\n",
    "        print(f\"    - {task.title} ({task.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261e35f",
   "metadata": {},
   "source": [
    "## 5. Create Task Runner to Execute Agents on Tasks\n",
    "\n",
    "Task runner that loops over all agent-task combinations, executes each task three times per agent, and collects outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMManager:\n",
    "    \"\"\"Manages LLM initialization and configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available_models = {\n",
    "            'gemini-2.0-flash-exp': self._create_gemini,\n",
    "            'gemini-1.5-pro': self._create_gemini, \n",
    "            'gpt-3.5-turbo': self._create_openai,\n",
    "            'gpt-4': self._create_openai,\n",
    "            'mistral-small': self._create_mistral,\n",
    "            'mistral-medium': self._create_mistral\n",
    "        }\n",
    "    \n",
    "    def _create_gemini(self, model_name: str, **kwargs) -> LLM:\n",
    "        \"\"\"Create Google Gemini model.\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "        \n",
    "        return GoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            google_api_key=api_key,\n",
    "            temperature=kwargs.get('temperature', 0.3),\n",
    "            max_output_tokens=kwargs.get('max_tokens', 2048)\n",
    "        )\n",
    "    \n",
    "    def _create_openai(self, model_name: str, **kwargs) -> LLM:\n",
    "        \"\"\"Create OpenAI model.\"\"\"\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "        \n",
    "        return OpenAI(\n",
    "            model=model_name,\n",
    "            openai_api_key=api_key,\n",
    "            temperature=kwargs.get('temperature', 0.3),\n",
    "            max_tokens=kwargs.get('max_tokens', 2048)\n",
    "        )\n",
    "    \n",
    "    def _create_mistral(self, model_name: str, **kwargs) -> LLM:\n",
    "        \"\"\"Create Mistral model.\"\"\"\n",
    "        api_key = os.getenv('MISTRAL_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"MISTRAL_API_KEY not found in environment variables\")\n",
    "        \n",
    "        return ChatMistralAI(\n",
    "            model=model_name,\n",
    "            mistral_api_key=api_key,\n",
    "            temperature=kwargs.get('temperature', 0.3),\n",
    "            max_tokens=kwargs.get('max_tokens', 2048)\n",
    "        )\n",
    "    \n",
    "    def create_llm(self, model_name: str, **kwargs) -> LLM:\n",
    "        \"\"\"Create an LLM instance.\"\"\"\n",
    "        if model_name not in self.available_models:\n",
    "            raise ValueError(f\"Model {model_name} not supported. Available: {list(self.available_models.keys())}\")\n",
    "        \n",
    "        return self.available_models[model_name](model_name, **kwargs)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Single experiment result.\"\"\"\n",
    "    timestamp: str\n",
    "    framework: str\n",
    "    task_id: str\n",
    "    task_type: str\n",
    "    run_number: int\n",
    "    success: bool\n",
    "    tokens_used: int\n",
    "    execution_time: float\n",
    "    memory_usage: float\n",
    "    reasoning_steps: int\n",
    "    final_answer: str\n",
    "    intermediate_steps: List[str]\n",
    "    validation_score: float\n",
    "    validation_passed: bool\n",
    "    validation_issues: List[str]\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Orchestrates the comparison experiment across all frameworks and tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'gemini-2.0-flash-exp', temperature: float = 0.3, runs_per_task: int = 3):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.runs_per_task = runs_per_task\n",
    "        \n",
    "        # Initialize components\n",
    "        self.llm_manager = LLMManager()\n",
    "        self.all_tasks = task_generator.get_all_tasks()\n",
    "        \n",
    "        # Agent classes\n",
    "        self.agent_classes = {\n",
    "            'react': ReActAgent,\n",
    "            'cot': CoTAgent,\n",
    "            'tot': ToTAgent\n",
    "        }\n",
    "        \n",
    "        print(f\"Experiment runner initialized:\")\n",
    "        print(f\"  Model: {self.model_name}\")\n",
    "        print(f\"  Temperature: {self.temperature}\")\n",
    "        print(f\"  Runs per task: {self.runs_per_task}\")\n",
    "        print(f\"  Frameworks: {list(self.agent_classes.keys())}\")\n",
    "    \n",
    "    def run_single_experiment(self, framework: str, task: Task, run_number: int) -> ExperimentResult:\n",
    "        \"\"\"Run a single experiment: one framework on one task.\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        try:\n",
    "            # Create LLM and agent\n",
    "            llm = self.llm_manager.create_llm(self.model_name, temperature=self.temperature)\n",
    "            agent_class = self.agent_classes[framework]\n",
    "            agent = agent_class(llm)\n",
    "            \n",
    "            # Execute task\n",
    "            metrics = agent.execute_task(task.prompt, task.task_type)\n",
    "            \n",
    "            # Simple validation (placeholder - would use real validators)\n",
    "            validation_passed = metrics.success and len(metrics.final_answer) > 50\n",
    "            validation_score = 75.0 if validation_passed else 25.0\n",
    "            validation_issues = [] if validation_passed else [\"Output too short or task failed\"]\n",
    "            \n",
    "            # Create result\n",
    "            result = ExperimentResult(\n",
    "                timestamp=timestamp,\n",
    "                framework=framework,\n",
    "                task_id=task.id,\n",
    "                task_type=task.task_type,\n",
    "                run_number=run_number,\n",
    "                success=metrics.success,\n",
    "                tokens_used=metrics.tokens_used,\n",
    "                execution_time=metrics.execution_time,\n",
    "                memory_usage=metrics.memory_usage,\n",
    "                reasoning_steps=metrics.reasoning_steps,\n",
    "                final_answer=metrics.final_answer,\n",
    "                intermediate_steps=metrics.intermediate_steps,\n",
    "                validation_score=validation_score,\n",
    "                validation_passed=validation_passed,\n",
    "                validation_issues=validation_issues,\n",
    "                error_message=metrics.error_message\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any unexpected errors\n",
    "            result = ExperimentResult(\n",
    "                timestamp=timestamp,\n",
    "                framework=framework,\n",
    "                task_id=task.id,\n",
    "                task_type=task.task_type,\n",
    "                run_number=run_number,\n",
    "                success=False,\n",
    "                tokens_used=0,\n",
    "                execution_time=0.0,\n",
    "                memory_usage=0.0,\n",
    "                reasoning_steps=0,\n",
    "                final_answer=\"\",\n",
    "                intermediate_steps=[],\n",
    "                validation_score=0.0,\n",
    "                validation_passed=False,\n",
    "                validation_issues=[f\"Experiment error: {str(e)}\"],\n",
    "                error_message=str(e)\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_quick_test(self) -> List[ExperimentResult]:\n",
    "        \"\"\"Run a quick test with one task per type and one run each.\"\"\"\n",
    "        print(\"Running quick test (1 task per type, 1 run each)...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Select first task from each type\n",
    "        for task_type, tasks in self.all_tasks.items():\n",
    "            if tasks:\n",
    "                task = tasks[0]  # First task of each type\n",
    "                print(f\"\\nTesting task: {task.title} ({task.id})\")\n",
    "                \n",
    "                for framework in self.agent_classes.keys():\n",
    "                    print(f\"  Framework: {framework.upper()}\", end=\" \")\n",
    "                    \n",
    "                    result = self.run_single_experiment(framework, task, 1)\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    status = \"✓\" if result.success else \"✗\"\n",
    "                    print(f\"- {status}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize experiment runner\n",
    "runner = ExperimentRunner()\n",
    "print(\"\\\\nTask runner ready for experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9147481",
   "metadata": {},
   "source": [
    "## 6. Implement Evaluation and Logging Utilities\n",
    "\n",
    "Utilities to check code correctness, validate itineraries, log token usage, latency, memory, and store outputs for manual interpretability scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a972d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultAnalyzer:\n",
    "    \"\"\"Analyzes and visualizes experiment results.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def results_to_dataframe(results: List[ExperimentResult]) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to pandas DataFrame for analysis.\"\"\"\n",
    "        data = []\n",
    "        for result in results:\n",
    "            data.append({\n",
    "                'timestamp': result.timestamp,\n",
    "                'framework': result.framework,\n",
    "                'task_id': result.task_id,\n",
    "                'task_type': result.task_type,\n",
    "                'run_number': result.run_number,\n",
    "                'success': result.success,\n",
    "                'tokens_used': result.tokens_used,\n",
    "                'execution_time': result.execution_time,\n",
    "                'memory_usage': result.memory_usage,\n",
    "                'reasoning_steps': result.reasoning_steps,\n",
    "                'validation_score': result.validation_score,\n",
    "                'validation_passed': result.validation_passed,\n",
    "                'error_message': result.error_message,\n",
    "                'answer_length': len(result.final_answer),\n",
    "                'steps_count': len(result.intermediate_steps)\n",
    "            })\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_success_rates(df: pd.DataFrame):\n",
    "        \"\"\"Plot success rates by framework.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Success rate by framework\n",
    "        success_by_framework = df.groupby('framework')['success'].mean()\n",
    "        ax1.bar(success_by_framework.index, success_by_framework.values)\n",
    "        ax1.set_title('Success Rate by Framework')\n",
    "        ax1.set_ylabel('Success Rate')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Success rate by task type\n",
    "        success_by_task = df.groupby('task_type')['success'].mean()\n",
    "        ax2.bar(success_by_task.index, success_by_task.values)\n",
    "        ax2.set_title('Success Rate by Task Type')\n",
    "        ax2.set_ylabel('Success Rate')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_performance_metrics(df: pd.DataFrame):\n",
    "        \"\"\"Plot performance metrics comparison.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Execution time\n",
    "        df.boxplot(column='execution_time', by='framework', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Execution Time by Framework')\n",
    "        axes[0,0].set_ylabel('Time (seconds)')\n",
    "        \n",
    "        # Token usage\n",
    "        df.boxplot(column='tokens_used', by='framework', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Token Usage by Framework')\n",
    "        axes[0,1].set_ylabel('Tokens')\n",
    "        \n",
    "        # Reasoning steps\n",
    "        df.boxplot(column='reasoning_steps', by='framework', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Reasoning Steps by Framework')\n",
    "        axes[1,0].set_ylabel('Steps')\n",
    "        \n",
    "        # Validation scores\n",
    "        df.boxplot(column='validation_score', by='framework', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Validation Scores by Framework')\n",
    "        axes[1,1].set_ylabel('Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_task_type_analysis(df: pd.DataFrame):\n",
    "        \"\"\"Plot analysis by task type and framework.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        task_types = df['task_type'].unique()\n",
    "        frameworks = df['framework'].unique()\n",
    "        \n",
    "        metrics = ['validation_score', 'execution_time', 'tokens_used']\n",
    "        titles = ['Validation Score', 'Execution Time', 'Token Usage']\n",
    "        \n",
    "        for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            pivot = df.groupby(['task_type', 'framework'])[metric].mean().unstack()\n",
    "            pivot.plot(kind='bar', ax=axes[i])\n",
    "            axes[i].set_title(f'{title} by Task Type and Framework')\n",
    "            axes[i].set_ylabel(metric.replace('_', ' ').title())\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].legend(title='Framework')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_summary_stats(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive summary statistics.\"\"\"\n",
    "        summary = {\n",
    "            'total_experiments': len(df),\n",
    "            'overall_success_rate': df['success'].mean(),\n",
    "            'avg_execution_time': df['execution_time'].mean(),\n",
    "            'avg_tokens_used': df['tokens_used'].mean(),\n",
    "            'avg_validation_score': df['validation_score'].mean(),\n",
    "        }\n",
    "        \n",
    "        # Framework-specific stats\n",
    "        framework_stats = {}\n",
    "        for framework in df['framework'].unique():\n",
    "            framework_df = df[df['framework'] == framework]\n",
    "            framework_stats[framework] = {\n",
    "                'success_rate': framework_df['success'].mean(),\n",
    "                'avg_execution_time': framework_df['execution_time'].mean(),\n",
    "                'avg_tokens_used': framework_df['tokens_used'].mean(),\n",
    "                'avg_validation_score': framework_df['validation_score'].mean(),\n",
    "                'avg_reasoning_steps': framework_df['reasoning_steps'].mean(),\n",
    "            }\n",
    "        \n",
    "        summary['framework_stats'] = framework_stats\n",
    "        \n",
    "        # Task type stats\n",
    "        task_type_stats = {}\n",
    "        for task_type in df['task_type'].unique():\n",
    "            task_df = df[df['task_type'] == task_type]\n",
    "            task_type_stats[task_type] = {\n",
    "                'success_rate': task_df['success'].mean(),\n",
    "                'avg_execution_time': task_df['execution_time'].mean(),\n",
    "                'avg_tokens_used': task_df['tokens_used'].mean(),\n",
    "                'avg_validation_score': task_df['validation_score'].mean(),\n",
    "            }\n",
    "        \n",
    "        summary['task_type_stats'] = task_type_stats\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_summary(summary: Dict[str, Any]):\n",
    "        \"\"\"Print formatted summary statistics.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EXPERIMENT SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"Total Experiments: {summary['total_experiments']}\")\n",
    "        print(f\"Overall Success Rate: {summary['overall_success_rate']:.1%}\")\n",
    "        print(f\"Average Validation Score: {summary['avg_validation_score']:.1f}\")\n",
    "        print(f\"Average Execution Time: {summary['avg_execution_time']:.2f}s\")\n",
    "        print(f\"Average Tokens Used: {summary['avg_tokens_used']:.0f}\")\n",
    "        \n",
    "        print(\"\\\\nFRAMEWORK COMPARISON:\")\n",
    "        print(\"-\" * 40)\n",
    "        for framework, stats in summary['framework_stats'].items():\n",
    "            print(f\"{framework.upper()}:\")\n",
    "            print(f\"  Success Rate: {stats['success_rate']:.1%}\")\n",
    "            print(f\"  Avg Score: {stats['avg_validation_score']:.1f}\")\n",
    "            print(f\"  Avg Time: {stats['avg_execution_time']:.2f}s\")\n",
    "            print(f\"  Avg Tokens: {stats['avg_tokens_used']:.0f}\")\n",
    "            print(f\"  Avg Steps: {stats['avg_reasoning_steps']:.1f}\")\n",
    "        \n",
    "        print(\"\\\\nTASK TYPE PERFORMANCE:\")\n",
    "        print(\"-\" * 40)\n",
    "        for task_type, stats in summary['task_type_stats'].items():\n",
    "            print(f\"{task_type.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Success Rate: {stats['success_rate']:.1%}\")\n",
    "            print(f\"  Avg Score: {stats['avg_validation_score']:.1f}\")\n",
    "            print(f\"  Avg Time: {stats['avg_execution_time']:.2f}s\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = ResultAnalyzer()\n",
    "print(\"Result analyzer ready for data analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e4daa",
   "metadata": {},
   "source": [
    "## 7. Run Experiments and Collect Results\n",
    "\n",
    "Execute the full experiment loop, saving results and logs to the /results/ directory for further analysis.\n",
    "\n",
    "**Note:** Before running experiments, make sure to:\n",
    "1. Set up your API keys in the `.env` file\n",
    "2. Configure your preferred LLM model\n",
    "3. Adjust the `runs_per_task` parameter as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check configuration before running experiments\n",
    "print(\"Configuration Check:\")\n",
    "print(f\"  GOOGLE_API_KEY: {'✓ Set' if os.getenv('GOOGLE_API_KEY') else '✗ Not set'}\")\n",
    "print(f\"  OPENAI_API_KEY: {'✓ Set' if os.getenv('OPENAI_API_KEY') else '✗ Not set'}\")\n",
    "print(f\"  MISTRAL_API_KEY: {'✓ Set' if os.getenv('MISTRAL_API_KEY') else '✗ Not set'}\")\n",
    "print(f\"  Available frameworks: {list(runner.agent_classes.keys())}\")\n",
    "print(f\"  Total tasks: {sum(len(tasks) for tasks in runner.all_tasks.values())}\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "print(f\"  Results directory: {results_dir.absolute()}\")\n",
    "\n",
    "print(\"\\\\n⚠️  Make sure at least one API key is set before running experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick test to verify everything works\n",
    "print(\"Running quick test experiment...\")\n",
    "print(\"This will test each framework on one task from each category.\")\n",
    "print(\"(Set DEMO_MODE=True to use mock results instead of real API calls)\")\n",
    "\n",
    "DEMO_MODE = True  # Set to False to use real API calls\n",
    "\n",
    "if DEMO_MODE:\n",
    "    print(\"\\\\n🔸 DEMO MODE: Using mock results (no API calls)\")\n",
    "    \n",
    "    # Create mock results for demonstration\n",
    "    demo_results = []\n",
    "    frameworks = ['react', 'cot', 'tot']\n",
    "    \n",
    "    for task_type, tasks in runner.all_tasks.items():\n",
    "        task = tasks[0]  # First task of each type\n",
    "        for framework in frameworks:\n",
    "            # Generate realistic mock metrics\n",
    "            base_time = {'react': 2.5, 'cot': 1.8, 'tot': 3.2}[framework]\n",
    "            base_tokens = {'react': 850, 'cot': 650, 'tot': 1200}[framework]\n",
    "            base_steps = {'react': 6, 'cot': 5, 'tot': 8}[framework]\n",
    "            \n",
    "            result = ExperimentResult(\n",
    "                timestamp=datetime.now().isoformat(),\n",
    "                framework=framework,\n",
    "                task_id=task.id,\n",
    "                task_type=task.task_type,\n",
    "                run_number=1,\n",
    "                success=True,\n",
    "                tokens_used=base_tokens + np.random.randint(-100, 200),\n",
    "                execution_time=base_time + np.random.uniform(-0.5, 1.0),\n",
    "                memory_usage=np.random.uniform(5, 15),\n",
    "                reasoning_steps=base_steps + np.random.randint(-2, 3),\n",
    "                final_answer=f\"Mock {framework} solution for {task.title}\",\n",
    "                intermediate_steps=[f\"Step {i+1}: Mock reasoning step\" for i in range(base_steps)],\n",
    "                validation_score=np.random.uniform(60, 95),\n",
    "                validation_passed=True,\n",
    "                validation_issues=[],\n",
    "                error_message=None\n",
    "            )\n",
    "            demo_results.append(result)\n",
    "    \n",
    "    quick_results = demo_results\n",
    "    print(f\"Generated {len(quick_results)} mock results\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\\\n🔸 LIVE MODE: Making real API calls\")\n",
    "    # Uncomment the next line to run real experiments\n",
    "    # quick_results = runner.run_quick_test()\n",
    "\n",
    "print(f\"\\\\nQuick test completed! Generated {len(quick_results)} results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = analyzer.results_to_dataframe(quick_results)\n",
    "\n",
    "print(\"Results DataFrame Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Frameworks: {df['framework'].unique()}\")\n",
    "print(f\"Task types: {df['task_type'].unique()}\")\n",
    "print(f\"Success rate: {df['success'].mean():.1%}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\\\nFirst 5 results:\")\n",
    "display(df[['framework', 'task_type', 'success', 'validation_score', 'execution_time', 'tokens_used']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations\n",
    "print(\"Generating performance analysis plots...\")\n",
    "\n",
    "# Success rates\n",
    "analyzer.plot_success_rates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55255ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics comparison\n",
    "analyzer.plot_performance_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task type analysis\n",
    "analyzer.plot_task_type_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b534090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display summary statistics\n",
    "summary = analyzer.generate_summary_stats(df)\n",
    "analyzer.print_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "csv_file = results_dir / f\"experiment_results_{timestamp}.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Results saved to: {csv_file}\")\n",
    "\n",
    "# Save summary statistics to JSON\n",
    "summary_file = results_dir / f\"summary_stats_{timestamp}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Summary saved to: {summary_file}\")\n",
    "\n",
    "# Save detailed results to JSON\n",
    "detailed_file = results_dir / f\"detailed_results_{timestamp}.json\"\n",
    "detailed_data = [asdict(result) for result in quick_results]\n",
    "with open(detailed_file, 'w') as f:\n",
    "    json.dump(detailed_data, f, indent=2)\n",
    "print(f\"Detailed results saved to: {detailed_file}\")\n",
    "\n",
    "print(\"\\\\n✅ Analysis complete! Files saved to results/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928eff1c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Running Full Experiments\n",
    "\n",
    "To run the complete experiment with all tasks and multiple runs:\n",
    "\n",
    "1. **Set up API keys** in your `.env` file:\n",
    "   ```\n",
    "   GOOGLE_API_KEY=your_key_here\n",
    "   OPENAI_API_KEY=your_key_here\n",
    "   MISTRAL_API_KEY=your_key_here\n",
    "   ```\n",
    "\n",
    "2. **Configure experiment parameters**:\n",
    "   - Model: Choose from available models (gemini-2.0-flash-exp, gpt-4, etc.)\n",
    "   - Temperature: 0.3 (recommended for consistency)\n",
    "   - Runs per task: 3 (for statistical significance)\n",
    "\n",
    "3. **Run full experiment**:\n",
    "   ```python\n",
    "   # Set DEMO_MODE = False in the experiment cell above\n",
    "   # OR use the command-line runner:\n",
    "   # python run_experiment.py\n",
    "   ```\n",
    "\n",
    "4. **Use Streamlit dashboard**:\n",
    "   ```bash\n",
    "   streamlit run streamlit_app.py\n",
    "   ```\n",
    "\n",
    "### Analysis Features\n",
    "\n",
    "This notebook provides:\n",
    "- ✅ **Framework Comparison**: ReAct vs CoT vs ToT\n",
    "- ✅ **Task Type Analysis**: Code generation, itinerary planning, procedure structuring  \n",
    "- ✅ **Performance Metrics**: Token usage, execution time, memory usage\n",
    "- ✅ **Success Rates**: Validation scores and pass/fail rates\n",
    "- ✅ **Visualization**: Charts and graphs for easy comparison\n",
    "- ✅ **Statistical Analysis**: Summary statistics and trends\n",
    "\n",
    "### Extending the Framework\n",
    "\n",
    "To add new reasoning frameworks:\n",
    "1. Create a new agent class inheriting from `BaseAgent`\n",
    "2. Implement `get_framework_prompt()` method\n",
    "3. Add extraction methods for reasoning steps and final answers\n",
    "4. Register in the `ExperimentRunner`\n",
    "\n",
    "To add new task types:\n",
    "1. Define tasks in `TaskGenerator`\n",
    "2. Create validation logic in `TaskValidator`\n",
    "3. Update visualization code as needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
