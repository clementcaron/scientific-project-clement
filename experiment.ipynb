{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6014f28",
   "metadata": {},
   "source": [
    "# LLM Reasoning Framework Comparison\n",
    "\n",
    "**Objective:** Compare ReAct, Chain-of-Thought, and Tree-of-Thoughts frameworks across three distinct task types.\n",
    "\n",
    "**Experiment Design:**\n",
    "- 1 task per domain (code, planning, structuring)\n",
    "- 3 frameworks √ó 3 runs = 9 experiments total\n",
    "- Automated evaluation + manual response analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6ed6a",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d02a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from agents import AgentFactory\n",
    "from tasks import TaskGenerator, TaskValidator\n",
    "from utils import LLMManager, ExperimentResult\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API configuration\n",
    "api_status = {\n",
    "    'Google': '‚úÖ' if os.getenv('GOOGLE_API_KEY') and not os.getenv('GOOGLE_API_KEY').startswith('your_') else '‚ùå'\n",
    "}\n",
    "\n",
    "print(\"API Configuration:\")\n",
    "for provider, status in api_status.items():\n",
    "    print(f\"  {provider}: {status}\")\n",
    "\n",
    "# Configuration\n",
    "MODEL = os.getenv('DEFAULT_MODEL')\n",
    "DEMO_MODE = True  # Set to False for real API calls\n",
    "\n",
    "print(f\"\\nModel: {MODEL}\")\n",
    "print(f\"Demo Mode: {'ON' if DEMO_MODE else 'OFF'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f73f8",
   "metadata": {},
   "source": [
    "## Task Definitions\n",
    "\n",
    "Three carefully selected tasks representing different cognitive demands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a178a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display our three tasks\n",
    "task_generator = TaskGenerator()\n",
    "all_tasks = task_generator.get_all_tasks()\n",
    "\n",
    "print(\"üìã EXPERIMENT TASKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for task_type, tasks in all_tasks.items():\n",
    "    task = tasks[0]  # We have exactly 1 task per type\n",
    "    print(f\"\\nüéØ {task_type.replace('_', ' ').title()}\")\n",
    "    print(f\"   ID: {task.id}\")\n",
    "    print(f\"   Title: {task.title}\")\n",
    "    print(f\"   Prompt: {task.prompt[:100]}...\")\n",
    "    print(f\"   Criteria: {len(task.validation_criteria)} validation points\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {sum(len(tasks) for tasks in all_tasks.values())} tasks loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad60d7",
   "metadata": {},
   "source": [
    "## Reasoning Frameworks\n",
    "\n",
    "**ReAct:** Combines reasoning and action in iterative cycles  \n",
    "**Chain-of-Thought:** Sequential step-by-step logical reasoning  \n",
    "**Tree-of-Thoughts:** Explores multiple reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize frameworks\n",
    "frameworks = AgentFactory.get_available_frameworks()\n",
    "llm_manager = LLMManager()\n",
    "\n",
    "print(\"üß† REASONING FRAMEWORKS\")\n",
    "print(\"=\" * 50)\n",
    "for framework in frameworks:\n",
    "    print(f\"‚úÖ {framework.upper()}\")\n",
    "\n",
    "print(f\"\\nüéØ Experiment Design: {len(frameworks)} frameworks √ó 1 task per type √ó 3 runs = {len(frameworks) * 3 * 3} total experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b103ee7",
   "metadata": {},
   "source": [
    "## Experiment Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13013ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(demo_mode=True, runs_per_task=3):\n",
    "    \"\"\"Run the complete experiment.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if demo_mode:\n",
    "        print(\"üî∏ DEMO MODE: Generating mock results (no API calls)\")\n",
    "        \n",
    "        # Generate realistic mock data\n",
    "        for task_type, tasks in all_tasks.items():\n",
    "            task = tasks[0]\n",
    "            for framework in frameworks:\n",
    "                for run in range(runs_per_task):\n",
    "                    # Realistic mock metrics\n",
    "                    base_scores = {'react': 80, 'cot': 75, 'tot': 85}\n",
    "                    base_times = {'react': 2.5, 'cot': 1.8, 'tot': 3.2}\n",
    "                    base_tokens = {'react': 850, 'cot': 650, 'tot': 1200}\n",
    "                    \n",
    "                    score = base_scores[framework] + (run * 2) + (hash(task.id) % 10 - 5)\n",
    "                    \n",
    "                    result = ExperimentResult(\n",
    "                        timestamp=datetime.now().isoformat(),\n",
    "                        framework=framework,\n",
    "                        task_id=task.id,\n",
    "                        task_type=task_type,\n",
    "                        run_number=run + 1,\n",
    "                        success=True,\n",
    "                        tokens_used=base_tokens[framework] + (run * 50),\n",
    "                        execution_time=base_times[framework] + (run * 0.3),\n",
    "                        memory_usage=8.5 + (run * 0.2),\n",
    "                        reasoning_steps=5 + run,\n",
    "                        final_answer=f\"Mock {framework.upper()} solution for {task.title} (Run {run+1}): This is a comprehensive response demonstrating the framework's approach...\",\n",
    "                        intermediate_steps=[f\"Step {i+1}: {framework} reasoning step\" for i in range(3+run)],\n",
    "                        validation_score=max(60, min(100, score)),\n",
    "                        validation_passed=score >= 70,\n",
    "                        validation_issues=[] if score >= 70 else [\"Mock validation issue\"],\n",
    "                        error_message=None\n",
    "                    )\n",
    "                    results.append(result)\n",
    "    else:\n",
    "        print(\"üî¥ LIVE MODE: Making real API calls\")\n",
    "        # Import and use the real experiment runner\n",
    "        from run_experiment import ExperimentRunner\n",
    "        runner = ExperimentRunner(model_name=MODEL, runs_per_task=runs_per_task)\n",
    "        results = runner.run_framework_comparison()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Experiment completed: {len(results)} results\")\n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "experiment_results = run_experiment(demo_mode=DEMO_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e28bf",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame([{\n",
    "    'framework': r.framework,\n",
    "    'task_type': r.task_type,\n",
    "    'task_id': r.task_id,\n",
    "    'run': r.run_number,\n",
    "    'success': r.success,\n",
    "    'score': r.validation_score,\n",
    "    'time': r.execution_time,\n",
    "    'tokens': r.tokens_used,\n",
    "    'steps': r.reasoning_steps\n",
    "} for r in experiment_results])\n",
    "\n",
    "print(\"üìä RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "print(f\"Success rate: {df['success'].mean():.1%}\")\n",
    "print(f\"Average score: {df['score'].mean():.1f}/100\")\n",
    "print(f\"Average time: {df['time'].mean():.1f}s\")\n",
    "print(f\"Average tokens: {df['tokens'].mean():.0f}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework comparison\n",
    "framework_stats = df.groupby('framework').agg({\n",
    "    'score': ['mean', 'std'],\n",
    "    'time': 'mean',\n",
    "    'tokens': 'mean',\n",
    "    'success': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"üèÜ FRAMEWORK COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "framework_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task type analysis\n",
    "task_stats = df.groupby('task_type').agg({\n",
    "    'score': ['mean', 'std'],\n",
    "    'time': 'mean',\n",
    "    'tokens': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"üìã TASK TYPE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "task_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ea43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('LLM Reasoning Framework Comparison', fontsize=16)\n",
    "\n",
    "# Score comparison\n",
    "sns.boxplot(data=df, x='framework', y='score', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation Scores by Framework')\n",
    "axes[0,0].set_ylabel('Score (0-100)')\n",
    "\n",
    "# Time comparison\n",
    "sns.barplot(data=df, x='framework', y='time', ax=axes[0,1])\n",
    "axes[0,1].set_title('Execution Time by Framework')\n",
    "axes[0,1].set_ylabel('Time (seconds)')\n",
    "\n",
    "# Task type performance\n",
    "sns.heatmap(df.pivot_table(values='score', index='framework', columns='task_type', aggfunc='mean'), \n",
    "            annot=True, cmap='RdYlGn', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Score by Framework & Task')\n",
    "\n",
    "# Token usage\n",
    "sns.scatterplot(data=df, x='tokens', y='score', hue='framework', size='time', ax=axes[1,1])\n",
    "axes[1,1].set_title('Score vs Token Usage')\n",
    "axes[1,1].set_xlabel('Tokens Used')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1b8e9",
   "metadata": {},
   "source": [
    "## Detailed Response Analysis\n",
    "\n",
    "Examine actual LLM outputs for qualitative insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed responses for each framework on each task\n",
    "print(\"üîç DETAILED RESPONSE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for task_type in df['task_type'].unique():\n",
    "    print(f\"\\nüìã Task: {task_type.replace('_', ' ').title()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    task_results = [r for r in experiment_results if r.task_type == task_type]\n",
    "    \n",
    "    # Show best run for each framework\n",
    "    for framework in frameworks:\n",
    "        framework_results = [r for r in task_results if r.framework == framework]\n",
    "        best_result = max(framework_results, key=lambda x: x.validation_score)\n",
    "        \n",
    "        print(f\"\\nüß† {framework.upper()} (Score: {best_result.validation_score:.0f}/100)\")\n",
    "        \n",
    "        # Show response preview\n",
    "        response_preview = best_result.final_answer[:200] + \"...\" if len(best_result.final_answer) > 200 else best_result.final_answer\n",
    "        print(f\"Response: {response_preview}\")\n",
    "        \n",
    "        # Show reasoning steps\n",
    "        if best_result.intermediate_steps:\n",
    "            print(f\"Reasoning Steps: {len(best_result.intermediate_steps)}\")\n",
    "            for i, step in enumerate(best_result.intermediate_steps[:2]):  # Show first 2 steps\n",
    "                print(f\"  {i+1}. {step[:80]}...\")\n",
    "        \n",
    "        # Show any issues\n",
    "        if best_result.validation_issues:\n",
    "            print(f\"Issues: {', '.join(best_result.validation_issues)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d894e9",
   "metadata": {},
   "source": [
    "## Key Insights & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "print(\"üí° KEY INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Best performing framework overall\n",
    "best_framework = df.groupby('framework')['score'].mean().idxmax()\n",
    "best_score = df.groupby('framework')['score'].mean().max()\n",
    "print(f\"üèÜ Best Overall Framework: {best_framework.upper()} (avg score: {best_score:.1f})\")\n",
    "\n",
    "# Most challenging task\n",
    "hardest_task = df.groupby('task_type')['score'].mean().idxmin()\n",
    "hardest_score = df.groupby('task_type')['score'].mean().min()\n",
    "print(f\"üéØ Most Challenging Task: {hardest_task.replace('_', ' ').title()} (avg score: {hardest_score:.1f})\")\n",
    "\n",
    "# Efficiency analysis\n",
    "efficiency = df.groupby('framework').apply(lambda x: x['score'].mean() / x['time'].mean()).round(2)\n",
    "most_efficient = efficiency.idxmax()\n",
    "print(f\"‚ö° Most Efficient Framework: {most_efficient.upper()} (score/time ratio: {efficiency.max():.1f})\")\n",
    "\n",
    "# Consistency analysis\n",
    "consistency = df.groupby('framework')['score'].std()\n",
    "most_consistent = consistency.idxmin()\n",
    "print(f\"üìä Most Consistent Framework: {most_consistent.upper()} (std dev: {consistency.min():.1f})\")\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE MATRIX:\")\n",
    "performance_matrix = df.pivot_table(values='score', index='framework', columns='task_type', aggfunc='mean').round(1)\n",
    "print(performance_matrix)\n",
    "\n",
    "print(\"\\nüî¨ STATISTICAL SUMMARY:\")\n",
    "print(f\"‚Ä¢ Score range: {df['score'].min():.1f} - {df['score'].max():.1f}\")\n",
    "print(f\"‚Ä¢ Time range: {df['time'].min():.1f}s - {df['time'].max():.1f}s\")\n",
    "print(f\"‚Ä¢ Token range: {df['tokens'].min():.0f} - {df['tokens'].max():.0f}\")\n",
    "print(f\"‚Ä¢ Overall success rate: {df['success'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a752b2e",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save summary CSV\n",
    "csv_file = results_dir / f\"experiment_summary_{timestamp}.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Save detailed JSON\n",
    "json_file = results_dir / f\"detailed_results_{timestamp}.json\"\n",
    "detailed_data = [{\n",
    "    'timestamp': r.timestamp,\n",
    "    'framework': r.framework,\n",
    "    'task_id': r.task_id,\n",
    "    'task_type': r.task_type,\n",
    "    'run_number': r.run_number,\n",
    "    'success': r.success,\n",
    "    'validation_score': r.validation_score,\n",
    "    'execution_time': r.execution_time,\n",
    "    'tokens_used': r.tokens_used,\n",
    "    'reasoning_steps': r.reasoning_steps,\n",
    "    'final_answer': r.final_answer,\n",
    "    'intermediate_steps': r.intermediate_steps,\n",
    "    'validation_issues': r.validation_issues,\n",
    "    'error_message': r.error_message\n",
    "} for r in experiment_results]\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(detailed_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved:\")\n",
    "print(f\"   Summary: {csv_file}\")\n",
    "print(f\"   Detailed: {json_file}\")\n",
    "print(f\"\\nüéØ Experiment complete! Check the results directory for full data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
